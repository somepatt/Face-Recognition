{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Создание Heatmaps для датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def create_heatmap(size, landmark, sigma=2):\n",
    "    \"\"\"\n",
    "    Создаёт один heatmap с гауссовым ядром вокруг точки.\n",
    "\n",
    "    :param size: (height, width) — размер heatmap'а\n",
    "    :param landmark:(x, y) — координаты точки\n",
    "    :param sigma\n",
    "    :return: heatmap массив\n",
    "    \"\"\"\n",
    "    x, y = landmark\n",
    "    h, w = size\n",
    "\n",
    "\n",
    "    x = min(max(0, int(x)), w - 1)\n",
    "    y = min(max(0, int(y)), h - 1)\n",
    "\n",
    "    xx, yy = np.meshgrid(np.arange(w), np.arange(h))\n",
    "    heatmap = np.exp(-((yy - y)**2 + (xx - x)**2) / (2 * sigma**2))\n",
    "    return heatmap\n",
    "\n",
    "\n",
    "def landmarks_to_heatmaps(image_shape, landmarks, sigma=2):\n",
    "    \"\"\"\n",
    "    Преобразует список из N точек в набор из N heatmap'ов.\n",
    "\n",
    "    :param image_shape: исходный размер изображения (H, W)\n",
    "    :param landmarks: список из N пар координат [(x1, y1), (x2, y2), ..., (xN, yN),]\n",
    "    :param sigma:\n",
    "    :return: массив heatmap'ов вида [N, H, W]\n",
    "    \"\"\"\n",
    "    heatmaps = []\n",
    "\n",
    "    for i in range(5):\n",
    "        x, y = landmarks[i]\n",
    "        hm = create_heatmap(image_shape, (x, y), sigma=sigma)\n",
    "        heatmaps.append(hm)\n",
    "\n",
    "    return np.array(heatmaps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Датасет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import albumentations as A\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "\n",
    "class Paths:\n",
    "    table_csv = '/kaggle/input/celeba-dataset/list_landmarks_align_celeba.csv'\n",
    "    root_dir = '/kaggle/input/celeba-dataset/img_align_celeba/img_align_celeba'\n",
    "\n",
    "\n",
    "class FaceDataset(Dataset):\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        self.annotations = pd.read_csv(csv_file).iloc[:10]\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.root_dir, self.annotations.iloc[idx, 0])\n",
    "        image = Image.open(img_name).convert('RGB')\n",
    "        image = np.array(image)\n",
    "\n",
    "        landmarks = np.array(self.annotations.iloc[idx, 1:].values.astype('float16')).reshape(-1, 2)\n",
    "\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image, keypoints=landmarks)\n",
    "            image = augmented['image']\n",
    "            landmarks = augmented['keypoints']\n",
    "\n",
    "        heatmaps = landmarks_to_heatmaps(image.shape[1:], landmarks)\n",
    "\n",
    "        return image, heatmaps\n",
    "    \n",
    "transform = A.Compose([\n",
    "    A.Resize(224, 224),\n",
    "    A.Normalize(),\n",
    "    ToTensorV2(),\n",
    "], keypoint_params=A.KeypointParams(format='xy', remove_invisible=False))\n",
    "\n",
    "\n",
    "path = Paths()\n",
    "\n",
    "data = FaceDataset(path.table_csv, path.root_dir, transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacked Hourglass Network + обучение\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.skip = nn.Identity() if in_channels == out_channels else nn.Conv2d(in_channels, out_channels, 1)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels // 2, 1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels // 2)\n",
    "        self.conv2 = nn.Conv2d(out_channels // 2, out_channels // 2, 3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels // 2)\n",
    "        self.conv3 = nn.Conv2d(out_channels // 2, out_channels, 1)\n",
    "        self.bn3 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = self.skip(x)\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.bn3(self.conv3(x))\n",
    "        return self.relu(x + residual)\n",
    "    \n",
    "\n",
    "class Hourglass(nn.Module):\n",
    "    def __init__(self, depth, num_features):\n",
    "        super().__init__()\n",
    "        self.depth = depth\n",
    "        self.num_features = num_features\n",
    "        self.downsample = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "\n",
    "        self.upper_branch = nn.ModuleList([ResidualBlock(num_features, num_features) for _ in range(depth)])\n",
    "        self.lower_branch = nn.ModuleList([ResidualBlock(num_features, num_features) for _ in range(depth)])\n",
    "        self.skip_branch = nn.ModuleList([ResidualBlock(num_features, num_features) for _ in range(depth)])\n",
    "\n",
    "    def forward(self, x, level=0):\n",
    "        if level == self.depth - 1:\n",
    "            return self.lower_branch[level](x)\n",
    "\n",
    "        up1 = self.upper_branch[level](x)\n",
    "        low1 = self.downsample(up1)\n",
    "        low2 = self.forward(low1, level + 1)\n",
    "        low3 = self.lower_branch[level](low2)\n",
    "        up2 = self.upsample(low3)\n",
    "\n",
    "        skip = self.skip_branch[level](x)\n",
    "        return up2 + skip\n",
    "    \n",
    "\n",
    "class StackedHourglass(nn.Module):\n",
    "    def __init__(self, num_stacks=2, num_features=256, num_keypoints=10):\n",
    "        super().__init__()\n",
    "        self.num_stacks = num_stacks\n",
    "        self.num_features = num_features\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, num_features // 2, kernel_size=7, stride=2, padding=3)\n",
    "        self.bn1 = nn.BatchNorm2d(num_features // 2)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.res1 = ResidualBlock(num_features // 2, num_features)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.hourglasses = nn.ModuleList([Hourglass(depth=4, num_features=num_features) for _ in range(num_stacks)])\n",
    "        self.residuals = nn.ModuleList([ResidualBlock(num_features, num_features) for _ in range(num_stacks)])\n",
    "        self.out_convs = nn.ModuleList([nn.Conv2d(num_features, num_keypoints, kernel_size=1) for _ in range(num_stacks)])\n",
    "        self.heatmap_convs = nn.ModuleList([nn.Conv2d(num_keypoints, num_features, kernel_size=1) for _ in range(num_stacks - 1)])\n",
    "        self.intermediate_convs = nn.ModuleList([nn.Conv2d(num_features, num_features, kernel_size=1) for _ in range(num_stacks - 1)])\n",
    "        self.intermediate_residuals = nn.ModuleList([ResidualBlock(num_features, num_features) for _ in range(num_stacks - 1)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = []\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.res1(x)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        for i in range(self.num_stacks):\n",
    "            hg = self.hourglasses[i](x)\n",
    "            res = self.residuals[i](hg)\n",
    "            out = self.out_convs[i](res)\n",
    "            out_upsampled = F.interpolate(out, size=(224, 224), mode='bilinear', align_corners=False)\n",
    "            outputs.append(out_upsampled)\n",
    "\n",
    "            if i < self.num_stacks - 1:\n",
    "                out_transformed = self.heatmap_convs[i](out)\n",
    "                x = x + self.intermediate_convs[i](res) + out_transformed\n",
    "                x = self.intermediate_residuals[i](x)\n",
    "\n",
    "        return outputs[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "class FaceLandmarksModel(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = StackedHourglass(2, num_features=256, num_keypoints=5)\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.transform = transform\n",
    "        self.photos = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch):\n",
    "        x, y = batch\n",
    "        x = x.to(self.device).float()\n",
    "        y = y.to(self.device).float()\n",
    "        logits = self(x)\n",
    "        loss = self.criterion(logits, y)\n",
    "        self.log('train_loss', loss, prog_bar=True)\n",
    "\n",
    "        current_lr = self.trainer.optimizers[0].param_groups[0]['lr']\n",
    "        self.log('lr', current_lr, prog_bar=True, on_step=True, on_epoch=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = self.criterion(logits, y)\n",
    "        self.log('val_loss', loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=1e-6)\n",
    "        scheduler = {\n",
    "            'scheduler': optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.1, patience=3),\n",
    "            'monitor': 'val_loss',\n",
    "            'interval': 'epoch',\n",
    "            'frequency': 1\n",
    "        }\n",
    "        return {'optimizer': optimizer, 'lr_scheduler': scheduler}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split, DataLoader\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "train_ds, val_ds = random_split(data, (0.8, 0.2))\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE)\n",
    "val_dl = DataLoader(val_ds, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    save_top_k=1,\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    filename='best-checkpoint',\n",
    "    save_last=True\n",
    ")\n",
    "\n",
    "model_shg = torch.load('/kaggle/input/shg95epoch/pytorch/default/1/sgh_95_epoch.pth', weights_only=False)\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=100, logger=True, callbacks=[checkpoint_callback])\n",
    "trainer.fit(model_shg, train_dl, val_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Загрузка модели для детекции лиц"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install facenet-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from facenet_pytorch import MTCNN\n",
    "from PIL import Image\n",
    "\n",
    "model4detect = MTCNN(\n",
    "    image_size=224,\n",
    "    thresholds=[0.5, 0.7, 0.8],\n",
    "    keep_all=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Загрузка + выгрузка модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_pl, 'sgh_95_epoch.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "model_pl = torch.load('/kaggle/input/shg95epoch/pytorch/default/1/sgh_95_epoch.pth', weights_only=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Функции для обработки логитов и фото"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import ImageDraw\n",
    "from typing import List\n",
    "from torch import tensor\n",
    "import cv2\n",
    "\n",
    "def visualize_heatmaps_on_image(image, model, transform, num_keypoints=5):\n",
    "    photo = image.convert('RGB').resize((224, 224))\n",
    "    photo_tensor = transform(image=np.array(photo))['image'].unsqueeze(0)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(photo_tensor)\n",
    "        heatmaps = outputs.squeeze(0).cpu().numpy()\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    fig.suptitle('Heatmaps Visualization', fontsize=16)\n",
    "    \n",
    "    axes[0, 0].imshow(photo)\n",
    "    axes[0, 0].set_title('Original Image')\n",
    "    axes[0, 0].axis('off')\n",
    "    \n",
    "    for i in range(num_keypoints):\n",
    "        row = i // 3\n",
    "        col = i % 3\n",
    "        \n",
    "        if row == 0 and col == 0:\n",
    "            continue\n",
    "        heatmap = heatmaps[i]\n",
    "        heatmap_normalized = (heatmap - heatmap.min()) / (heatmap.max() - heatmap.min() + 1e-8)\n",
    "        \n",
    "        heatmap_colored = plt.cm.jet(heatmap_normalized)[:, :, :3]\n",
    "\n",
    "        img_array = np.array(photo) / 255.0\n",
    "        overlay = 0.7 * heatmap_colored + 0.3 * img_array\n",
    "        \n",
    "        axes[row, col].imshow(overlay)\n",
    "        axes[row, col].set_title(f'Keypoint {i+1} Heatmap')\n",
    "        axes[row, col].axis('off')\n",
    "    \n",
    "    if num_keypoints < 5:\n",
    "        axes[1, 2].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return heatmaps\n",
    "\n",
    "def visualize_combined_heatmap(image, model, transform, num_keypoints=5):\n",
    "    photo = np.array(image.convert('RGB').resize((224, 224)))\n",
    "    photo_tensor = transform(image=photo)['image'].unsqueeze(0)\n",
    "    \n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(photo_tensor)\n",
    "        heatmaps = outputs.squeeze(0).cpu().numpy()\n",
    "\n",
    "\n",
    "    \n",
    "    combined_heatmap = np.zeros((224, 224))\n",
    "    for i in range(num_keypoints):\n",
    "        heatmap_normalized = (heatmaps[i] - heatmaps[i].min()) / (heatmaps[i].max() - heatmaps[i].min() + 1e-8)\n",
    "        combined_heatmap += heatmap_normalized\n",
    "    \n",
    "    combined_heatmap = (combined_heatmap - combined_heatmap.min()) / (combined_heatmap.max() - combined_heatmap.min() + 1e-8)\n",
    "    \n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    ax1.imshow(photo)\n",
    "    ax1.set_title('Original Image')\n",
    "    ax1.axis('off')\n",
    "    \n",
    "    ax2.imshow(combined_heatmap, cmap='jet')\n",
    "    ax2.set_title('Combined Heatmap')\n",
    "    ax2.axis('off')\n",
    "    \n",
    "    img_array = np.array(photo) / 255.0\n",
    "    heatmap_colored = plt.cm.jet(combined_heatmap)[:, :, :3]\n",
    "    overlay = 0.7 * heatmap_colored + 0.3 * img_array\n",
    "    \n",
    "    ax3.imshow(overlay)\n",
    "    ax3.set_title('Overlay')\n",
    "    ax3.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return combined_heatmap\n",
    "\n",
    "def extract_keypoints_from_heatmaps(heatmaps):\n",
    "    keypoints = []\n",
    "    for heatmap in heatmaps:\n",
    "        y, x = np.unravel_index(np.argmax(heatmap), heatmap.shape)\n",
    "        keypoints.append((x, y))\n",
    "    return keypoints\n",
    "\n",
    "def draw_keypoints_on_image(image, model, transform):\n",
    "    photo = image.convert('RGB').resize((224 , 224))\n",
    "    photo_tensor = transform(image=np.array(photo))['image'].unsqueeze(0)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(photo_tensor)\n",
    "        heatmaps = outputs.squeeze(0).cpu().numpy()\n",
    "    keypoints = extract_keypoints_from_heatmaps(heatmaps)\n",
    "    \n",
    "    draw = ImageDraw.Draw(photo)\n",
    "    color = 'red'\n",
    "    \n",
    "    for i, (x, y) in enumerate(keypoints):\n",
    "        r = 2\n",
    "        draw.ellipse((x - r, y - r, x + r, y + r), fill=color, outline='white')\n",
    "        draw.text((x + 8, y - 8), str(i+1), fill='white')\n",
    "    \n",
    "    return photo\n",
    "\n",
    "def get_key_points(image, model, transform):\n",
    "    photo = image.convert('RGB').resize((224 , 224))\n",
    "    photo_tensor = transform(image=np.array(photo))['image'].unsqueeze(0)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(photo_tensor)\n",
    "        heatmaps = outputs.squeeze(0).cpu().numpy()\n",
    "    keypoints = extract_keypoints_from_heatmaps(heatmaps)\n",
    "    return keypoints\n",
    "\n",
    "def get_bboxes_faces(image: Image, model) -> List[tensor] | None:\n",
    "    boxes, _ = model.detect(image)\n",
    "\n",
    "    if boxes is not None:\n",
    "        return boxes\n",
    "    \n",
    "    raise AssertionError('Лица не были найдены')\n",
    "\n",
    "def get_cropped_faces(image: Image, boxes: List[List]) -> List[tensor]:\n",
    "    size = image.size\n",
    "\n",
    "    cropped_faces = []\n",
    "    for box in boxes:\n",
    "        x1, y1, x2, y2 = box\n",
    "        w, h = x2 - x1, y2 - y1\n",
    "\n",
    "        w_margin = 0.75 * w\n",
    "        h_margin = 0.65 * h\n",
    "\n",
    "        new_x1 = max(0, x1 - w_margin)\n",
    "        new_y1 = max(0, y1 - h_margin)\n",
    "        new_x2 = min(image.width, x2 + w_margin)\n",
    "        new_y2 = min(image.height, y2 + h_margin)\n",
    "        cropped_faces.append(image.crop((new_x1, new_y1, new_x2, new_y2)))\n",
    "\n",
    "\n",
    "    return cropped_faces\n",
    "\n",
    "def get_bboxes_faces(image: Image, model) -> List[tensor] | None:\n",
    "    boxes, _ = model.detect(image)\n",
    "\n",
    "    if boxes is not None:\n",
    "        return boxes\n",
    "    \n",
    "    raise AssertionError('Лица не были найдены')\n",
    "\n",
    "def get_cropped_faces(image: Image, boxes: List[List]) -> List[tensor]:\n",
    "    size = image.size\n",
    "\n",
    "    cropped_faces = []\n",
    "    for box in boxes:\n",
    "        x1, y1, x2, y2 = box\n",
    "        w, h = x2 - x1, y2 - y1\n",
    "\n",
    "        w_margin = 0.75 * w\n",
    "        h_margin = 0.65 * h\n",
    "\n",
    "        new_x1 = max(0, x1 - w_margin)\n",
    "        new_y1 = max(0, y1 - h_margin)\n",
    "        new_x2 = min(image.width, x2 + w_margin)\n",
    "        new_y2 = min(image.height, y2 + h_margin)\n",
    "        cropped_faces.append(image.crop((new_x1, new_y1, new_x2, new_y2)))\n",
    "\n",
    "\n",
    "    return cropped_faces\n",
    "\n",
    "def aligned_image(image, model, transform):\n",
    "    kp = get_key_points(image, model, transform)\n",
    "\n",
    "    src_points = np.array([\n",
    "        kp[0],\n",
    "        kp[1],\n",
    "        kp[2]\n",
    "    ], dtype=np.float32)\n",
    "\n",
    "    dst_points = np.array([\n",
    "        [60, 80],\n",
    "        [160, 80],\n",
    "        [110, 130]\n",
    "    ], dtype=np.float32)\n",
    "\n",
    "    M = cv2.getAffineTransform(src_points, dst_points)\n",
    "\n",
    "    aligned_image = cv2.warpAffine(np.array(image), M, (224, 224))\n",
    "    return Image.fromarray(aligned_image)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Тест пайплайна(детекция + landmarks/align)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open('/kaggle/input/ufgdsauigisda/photo_2024-12-31_13-04-42.jpg')\n",
    "boxes = get_bboxes_faces(img)\n",
    "face = get_cropped_faces(img, boxes)[0]\n",
    "draw_keypoints_on_image(face, model_pl, transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open('/kaggle/input/celeba-dataset/img_align_celeba/img_align_celeba/000003.jpg')\n",
    "boxes = get_bboxes_faces(img, model4detect)\n",
    "face = get_cropped_faces(img, boxes)[0]\n",
    "aligned_image(face, model_shg, transform)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
